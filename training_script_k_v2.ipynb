{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_script_k_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJve5DYOsGJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b197ca94-b107-4bff-e08b-e60a4d3c85e9"
      },
      "source": [
        "!ls\n",
        "!ls drive/My\\ Drive/UFBA/Doutorado/Ssd_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "checkpoints  Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIMM-dqOskzy",
        "colab_type": "text"
      },
      "source": [
        "# Instalações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS91bSiUsj5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79455f29-04d9-419c-c77a-2bf8f5887608"
      },
      "source": [
        "!pip install neptune-client\n",
        "!pip install mxnet-cu101\n",
        "#==1.5.0\n",
        "!pip install gluoncv\n",
        "#==0.7.0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.6/dist-packages (0.4.119)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.6/dist-packages (from neptune-client) (0.2.6)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (0.18.2)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (3.1.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (3.1.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.0.5)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (7.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.15.0)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.6/dist-packages (from neptune-client) (10.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client) (20.4)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (0.57.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.6/dist-packages (from neptune-client) (1.7.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from py3nvml->neptune-client) (0.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client) (2.8.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.17.2)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.7.4.3)\n",
            "Requirement already satisfied: msgpack-python in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (0.5.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (5.17.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client) (2020.6.20)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.6.0)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (0.2)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.7.3)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (1.0.0)\n",
            "Requirement already satisfied: rfc3987; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: webcolors; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client) (1.11.1)\n",
            "Requirement already satisfied: strict-rfc3339; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client) (0.7)\n",
            "Requirement already satisfied: mxnet-cu101 in /usr/local/lib/python3.6/dist-packages (1.6.0.post0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.18.5)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Requirement already satisfied: gluoncv in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gluoncv) (2.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gluoncv) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gluoncv) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gluoncv) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from gluoncv) (3.2.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from gluoncv) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gluoncv) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->gluoncv) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->gluoncv) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5L8-6kass9i",
        "colab_type": "text"
      },
      "source": [
        "# Importações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUIze8EmswyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import argparse\n",
        "\n",
        "# disable autotune\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "from mxnet import nd\n",
        "from mxnet import gluon\n",
        "from mxnet import autograd\n",
        "import gluoncv as gcv\n",
        "from gluoncv import data as gdata\n",
        "from gluoncv import utils as gutils\n",
        "from gluoncv.model_zoo import get_model\n",
        "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
        "##<FASTER\n",
        "from gluoncv.data.batchify import FasterRCNNTrainBatchify, Append # ,Tuple \n",
        "from gluoncv.data.transforms.presets.rcnn import FasterRCNNDefaultTrainTransform, \\\n",
        "    FasterRCNNDefaultValTransform\n",
        "from gluoncv.utils.parallel import Parallel\n",
        "from gluoncv.utils.metrics.rcnn import RPNAccMetric, RPNL1LossMetric, RCNNAccMetric, \\\n",
        "    RCNNL1LossMetric\n",
        "from gluoncv.model_zoo.rcnn.faster_rcnn.data_parallel import ForwardBackwardTask\n",
        "##/>\n",
        "from gluoncv.data.transforms.presets.yolo import YOLO3DefaultTrainTransform\n",
        "from gluoncv.data.transforms.presets.yolo import YOLO3DefaultValTransform\n",
        "from gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform\n",
        "from gluoncv.data.transforms.presets.ssd import SSDDefaultValTransform\n",
        "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric\n",
        "from gluoncv.utils.metrics.coco_detection import COCODetectionMetric\n",
        "import gluoncv.data.transforms.bbox as tbbox\n",
        "import gluoncv.data.transforms.image as timage\n",
        "import gluoncv.data.transforms.experimental as experimental\n",
        "import cv2\n",
        "from gluoncv.utils.bbox import bbox_iou \n",
        "# from mxnet.contrib import amp\n",
        "\n",
        "import neptune\n",
        "os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'\n",
        "os.environ['NEPTUNE_API_TOKEN'] = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiM2UxMWNkZTgtODZhZi00MWFmLWE3ZDEtOWYwZTliMzk4ZDFmIn0='\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q7rrf6Ws4kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSDCustomValTransform(object):\n",
        "    \"\"\"Default SSD training transform which includes tons of image augmentations.\"\"\"\n",
        "\n",
        "    def __init__(self, width, height, anchors=None, mean=(0.485, 0.456, 0.406),\n",
        "                 std=(0.229, 0.224, 0.225), iou_thresh=0.5, box_norm=(0.1, 0.1, 0.2, 0.2),\n",
        "                 **kwargs):\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._anchors = anchors\n",
        "        self._mean = mean\n",
        "        self._std = std\n",
        "        if anchors is None:\n",
        "            return\n",
        "\n",
        "        # since we do not have predictions yet, so we ignore sampling here\n",
        "        from gluoncv.model_zoo.ssd.target import SSDTargetGenerator\n",
        "        self._target_generator = SSDTargetGenerator(\n",
        "            iou_thresh=iou_thresh, stds=box_norm, negative_mining_ratio=-1, **kwargs)\n",
        "\n",
        "    def __call__(self, src, label):\n",
        "        \"\"\"Apply transform to validation image/label.\"\"\"\n",
        "        # resize with random interpolation\n",
        "        h, w, _ = src.shape\n",
        "        img = timage.imresize(src, self._width, self._height, interp=9)\n",
        "        bbox = tbbox.resize(label, (w, h), (self._width, self._height))\n",
        "\n",
        "        # to tensor\n",
        "        img = mx.nd.image.to_tensor(img)\n",
        "        img = mx.nd.image.normalize(img, mean=self._mean, std=self._std)\n",
        "\n",
        "        if self._anchors is None:\n",
        "            return img, bbox.astype(img.dtype)\n",
        "\n",
        "        # generate training target so cpu workers can help reduce the workload on gpu\n",
        "        gt_bboxes = mx.nd.array(bbox[np.newaxis, :, :4])\n",
        "        gt_ids = mx.nd.array(bbox[np.newaxis, :, 4:5])\n",
        "        cls_targets, box_targets, _ = self._target_generator(\n",
        "            self._anchors, None, gt_bboxes, gt_ids)\n",
        "        return img, cls_targets[0], box_targets[0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e5ZnCJLuHR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwJCO-Zs7Hf",
        "colab_type": "text"
      },
      "source": [
        "# SSD and YOLO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni4J65SatJl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class training_network():\n",
        "    def __init__(self, model='ssd300', ctx='gpu', resume_training=False, batch_size=4, num_workers=2, lr=0.001, \n",
        "                 lr_decay=0.1, lr_decay_epoch='60, 80', wd=0.0005, momentum=0.9, start_epoch=0,\n",
        "                 epochs=2, dataset='voc', network='vgg16_atrous', resume='',\n",
        "                 beta1=0.9, beta2=0.999, epsilon=1e-08, validation_threshold=0.5, nms_threshold=0.5, optimizer='sgd', x_id=None):\n",
        "        \n",
        "        # exp=False\n",
        "        \"\"\"\n",
        "        Script responsible for training the class\n",
        "\n",
        "        Arguments:\n",
        "            val_interval (int, default: 1): Epoch interval for validation, increase the number will reduce the\n",
        "                training time if validation is slow.\n",
        "            wd (float, default: 0.0005): Weight decay, default is 5e-4\n",
        "            momentum (float, default:0.9): SGD momentum, default is 0.9\n",
        "            lr_decay_epoch (str, default: '60, 80'): epoches at which learning rate decays. default is 60, 80.\n",
        "            lr_decay (float, default: 0.1): decay rate of learning rate. default is 0.1.\n",
        "            lr (float, default: 0.001): Learning rate, default is 0.001\n",
        "            start_epoch (int, default: 0): Starting epoch for resuming, default is 0 for new training. You can\n",
        "                specify it to 100 for example to start from 100 epoch.\n",
        "            resume (str, default: ''): Resume from previously saved parameters if not None. For example, you \n",
        "                can resume from ./ssd_xxx_0123.params'\n",
        "            epochs (int, default:2): Training epochs.\n",
        "            num_worker (int, default: 2): number to accelerate data loading\n",
        "            dataset (str, default:'voc'): Training dataset. Now support voc.\n",
        "            batch_size (int, default: 4): Training mini-batch size\n",
        "            data_shape (int, default: 300): Input data shape, use 300, 512.\n",
        "            network (str, default:'vgg16_atrous'): Base network name which serves as feature extraction base.\n",
        "        \"\"\"\n",
        "\n",
        "        # amp.init()\n",
        "\n",
        "        # TRAINING PARAMETERS\n",
        "        self.net_type='SSD_or_YOLO'\n",
        "        self.x_id = x_id\n",
        "        self.old_save = None\n",
        "        self.resume_training = resume_training\n",
        "        self.batch_size=batch_size\n",
        "        self.num_workers=num_workers\n",
        "        self.learning_rate = lr\n",
        "        self.weight_decay = wd\n",
        "        self.momentum = momentum\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr_decay_epoch = lr_decay_epoch\n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.resume = resume\n",
        "        self.validation_threshold = validation_threshold\n",
        "        self.nms_threshold = nms_threshold\n",
        "        # self.experiment = experiment\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.epsilon=epsilon\n",
        "        self.best_map = 0\n",
        "        self.model = model.lower()\n",
        "\n",
        "        if ctx == 'cpu':\n",
        "            self.ctx = [mx.cpu()]\n",
        "        elif ctx == 'gpu':\n",
        "            self.ctx = [mx.gpu(0)]\n",
        "        else:\n",
        "            raise ValueError('Invalid context.')\n",
        "            \n",
        "        # fix seed for mxnet, numpy and python builtin random generator.\n",
        "        gutils.random.seed(233)\n",
        "\n",
        "        if model.lower() == 'ssd_300_vgg16_atrous_voc':\n",
        "            self.network = 'ssd'\n",
        "            self.net_type = 'ssd'\n",
        "            self.width, self.height = 300, 300\n",
        "        elif model.lower() == 'ssd_512_resnet50_v1_voc':\n",
        "            self.network = 'ssd'\n",
        "            self.net_type = 'ssd'\n",
        "            self.width, self.height = 512, 512\n",
        "        elif model.lower() == 'ssd_512_vgg16_atrous_voc':\n",
        "            self.network = 'ssd'\n",
        "            self.net_type = 'ssd'\n",
        "            self.width, self.height = 512, 512\n",
        "        elif model.lower() == 'ssd_300_vgg16_atrous_coco':\n",
        "            self.network = 'ssd'\n",
        "            self.net_type = 'ssd'\n",
        "            self.width, self.height = 300, 300\n",
        "        elif model.lower() == 'ssd_512_vgg16_atrous_coco':\n",
        "            self.network = 'ssd'\n",
        "            self.net_type = 'ssd'\n",
        "            self.width, self.height = 512, 512\n",
        "        elif model.lower() == 'ssd_512_resnet50_v1_coco':\n",
        "            self.network = 'ssd'\n",
        "            self.net_type = 'ssd'\n",
        "            self.width, self.height = 512, 512\n",
        "        elif model.lower() == 'yolo3_darknet53_voc':\n",
        "            self.network = 'yolo'\n",
        "            self.net_type= 'yolo'\n",
        "            self.width, self.height = 320, 320\n",
        "        elif model.lower() == 'yolo3_darknet53_coco':\n",
        "            self.network = 'yolo'\n",
        "            self.net_type= 'yolo'\n",
        "            self.width, self.height = 320, 320\n",
        "        else:\n",
        "            raise ValueError('Invalid model `{}`.'.format(model.lower()))\n",
        "                \n",
        "        # TODO: Specify the checkpoints save path\n",
        "        self.save_prefix = 'drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints'\n",
        "        \n",
        "        # train and val rec file\n",
        "        self.train_file = 'drive/My Drive/UFBA/Doutorado/Ssd_test/Dataset/train_teste_7_300_300.rec'\n",
        "        self.val_file = 'drive/My Drive/UFBA/Doutorado/Ssd_test/Dataset/val_teste_7_300_300.rec'\n",
        "\n",
        "        self.classes = ['bar_clamp', 'gear_box', 'vase', 'part_1', 'part_3', 'nozzle', 'pawn', 'turbine_housing'] # please, follow the order of the config.json file\n",
        "        print('Classes: ', self.classes)\n",
        "\n",
        "        # pretrained or pretrained_base?\n",
        "        # pretrained (bool or str) – Boolean value controls whether to load the default \n",
        "        # pretrained weights for model. String value represents the hashtag for a certain \n",
        "        # version of pretrained weights.\n",
        "        # pretrained_base (bool or str, optional, default is True) – Load pretrained base \n",
        "        # network, the extra layers are randomized. Note that if pretrained is True, this\n",
        "        # has no effect.\n",
        "        self.net = get_model(model, pretrained=True, norm_layer=gluon.nn.BatchNorm)\n",
        "        self.net.reset_class(self.classes)\n",
        "        \n",
        "        # Initialize the weights\n",
        "        if self.resume_training:\n",
        "            self.net.initialize(force_reinit=True, ctx=self.ctx)\n",
        "            self.net.load_params(self.resume, ctx=self.ctx)\n",
        "        else:\n",
        "            for param in self.net.collect_params().values():\n",
        "                if param._data is not None:\n",
        "                    continue\n",
        "                param.initialize()\n",
        "\n",
        "\n",
        "    def get_dataset(self):\n",
        "        validation_threshold = self.validation_threshold\n",
        "        self.train_dataset = gdata.RecordFileDetection(self.train_file)\n",
        "        self.val_dataset = gdata.RecordFileDetection(self.val_file)\n",
        "        # we are only using VOCMetric for evaluation\n",
        "        if not self.net_type =='faster':\n",
        "          self.val_metric = VOC07MApMetric(iou_thresh=validation_threshold, class_names=self.net.classes)\n",
        "        else: \n",
        "          self.val_metric = VOC07MApMetric(iou_thresh=validation_threshold, class_names=self.classes)\n",
        "\n",
        "\n",
        "    def show_summary(self):\n",
        "        self.net.summary(mx.nd.ones((1, 3, self.height, self.width)))\n",
        "\n",
        "    def get_dataloader(self):\n",
        "        width, height = self.width, self.height\n",
        "        train_dataset = self.train_dataset\n",
        "        val_dataset = self.val_dataset\n",
        "        batch_size = self.batch_size\n",
        "        num_workers = self.num_workers\n",
        "        network = self.network\n",
        "        if network == 'ssd':\n",
        "            # use fake data to generate fixed anchors for target generation\n",
        "            with autograd.train_mode():\n",
        "                _, _, anchors = self.net(mx.nd.zeros((1, 3, height, width)))\n",
        "\n",
        "            batchify_fn = Tuple(Stack(), Stack(), Stack())  # stack image, cls_targets, box_targets\n",
        "            train_loader = gluon.data.DataLoader(train_dataset.transform(SSDDefaultTrainTransform(width, height, anchors)),\n",
        "                                                 batch_size, True, \n",
        "                                                 batchify_fn=batchify_fn, \n",
        "                                                 last_batch='rollover', \n",
        "                                                 num_workers=num_workers)\n",
        "\n",
        "            # Val verdadeiro\n",
        "            val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
        "            val_loader = gluon.data.DataLoader(val_dataset.transform(SSDDefaultValTransform(width, height)),\n",
        "                                               batch_size, False, \n",
        "                                               batchify_fn=val_batchify_fn, \n",
        "                                               last_batch='keep', \n",
        "                                               num_workers=num_workers)\n",
        "          \n",
        "            # use fake data to generate fixed anchors for target generation\n",
        "            with mx.Context(mx.gpu(0)):\n",
        "                anchors2 = anchors\n",
        "\n",
        "            val_loader_loss = gluon.data.DataLoader(val_dataset.transform(SSDCustomValTransform(width, height, anchors2)),\n",
        "                                                    batch_size, True, \n",
        "                                                    batchify_fn=batchify_fn, \n",
        "                                                    last_batch='rollover', \n",
        "                                                    num_workers=num_workers)\n",
        "            self.val_loader_loss = val_loader_loss\n",
        "        elif network == 'yolo':\n",
        "            batchify_fn = Tuple(*([Stack() for _ in range(6)] + [Pad(axis=0, pad_val=-1) for _ in range(1)]))  # stack image, all targets generated\n",
        "            # if args.no_random_shape:\n",
        "            train_loader = gluon.data.DataLoader(train_dataset.transform(YOLO3DefaultTrainTransform(width, height, self.net)),\n",
        "                                                 batch_size, True, \n",
        "                                                 batchify_fn=batchify_fn, \n",
        "                                                 last_batch='rollover', \n",
        "                                                 num_workers=num_workers)\n",
        "\n",
        "            val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
        "            val_loader = gluon.data.DataLoader(val_dataset.transform(YOLO3DefaultValTransform(width, height)),\n",
        "                                               batch_size, False, \n",
        "                                               batchify_fn=val_batchify_fn, \n",
        "                                               last_batch='keep', \n",
        "                                               num_workers=num_workers)\n",
        "        else:\n",
        "            raise ValueError(\"Network {} not implemented\".format(network))\n",
        "\n",
        "        self.val_loader = val_loader\n",
        "        self.train_loader = train_loader\n",
        "\n",
        "    # def save_params(self, current_map, epoch):\n",
        "    #     prefix = self.save_prefix\n",
        "    #     best_map = self.best_map\n",
        "\n",
        "    #     pre = self.save_prefix,\n",
        "    #     print(pre) \n",
        "\n",
        "    #     current_map = float(current_map)        \n",
        "    #     if current_map > best_map:\n",
        "    #         best_map = current_map\n",
        "    #         self.net.save_parameters('/{:s}_best_epoch_{:04d}_map_{:.4f}.params'.format(prefix, epoch, current_map))\n",
        "        \n",
        "    #     self.best_map = best_map\n",
        "    #     print('Best map: ', self.best_map)\n",
        "\n",
        "\n",
        "    def save_params(self, current_map, epoch):\n",
        "        prefix = self.save_prefix #self.save_prefix\n",
        "        pre = '{:s}/{:s}/{:s}/'.format(prefix, self.net_type, self.x_id)\n",
        "        print(prefix)\n",
        "        best_map = self.best_map\n",
        "\n",
        "        if not os.path.exists(pre):\n",
        "          os.makedirs(pre)\n",
        "\n",
        "        current_map = float(current_map)        \n",
        "        if current_map > best_map:\n",
        "            best_map = current_map\n",
        "            cur_save= '{:s}{:s}_BE_{:04d}_map_{:.4f}.params'.format(pre, self.model, epoch, current_map) \n",
        "            print(cur_save)\n",
        "            self.net.save_parameters(cur_save)\n",
        "            if not (self.old_save == None):\n",
        "              open(self.old_save, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "              os.remove(self.old_save) #delete the blank file from google drive will move the file to bin instead\n",
        "            self.old_save = (cur_save)\n",
        "        \n",
        "        self.best_map = best_map\n",
        "        print('Best map: ', self.best_map)\n",
        "\n",
        "    def val_loss(self):\n",
        "        \"\"\"Training pipeline\"\"\"        \n",
        "        val_data = self.val_loader_loss\n",
        "        ctx = self.ctx\n",
        "        val_metric = self.val_metric\n",
        "\n",
        "        mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
        "        ce_metric = mx.metric.Loss('CrossEntropy')\n",
        "        smoothl1_metric = mx.metric.Loss('SmoothL1')\n",
        "\n",
        "        ce_metric.reset() # Resets the internal evaluation result to initial state.\n",
        "        smoothl1_metric.reset() # Resets the internal evaluation result to initial state.\n",
        "\n",
        "        for i, batch in enumerate(val_data):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
        "            cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
        "            box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
        "\n",
        "            cls_preds = []\n",
        "            box_preds = []\n",
        "            for x in data:\n",
        "                cls_pred, box_pred, _ = self.net(x)\n",
        "                cls_preds.append(cls_pred)\n",
        "                box_preds.append(box_pred)\n",
        "                # descobrir o id de cada ifnerência pra usar no iou\n",
        "            \n",
        "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
        "                cls_preds, box_preds, cls_targets, box_targets)\n",
        "        \n",
        "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
        "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
        "\n",
        "        name1, loss1 = ce_metric.get()\n",
        "        name2, loss2 = smoothl1_metric.get()\n",
        "\n",
        "        return name1, loss1, name2, loss2\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Test on validation dataset.\"\"\"\n",
        "        val_data = self.val_loader\n",
        "        ctx = self.ctx\n",
        "        val_metric = self.val_metric\n",
        "        nms_threshold = self.nms_threshold\n",
        "        validation_threshold = self.validation_threshold\n",
        "\n",
        "        val_metric.reset()\n",
        "        # set nms threshold and topk constraint\n",
        "        # post_nms = maximum number of objects per image\n",
        "        self.net.set_nms(nms_thresh=nms_threshold, nms_topk=200, post_nms=len(self.classes)) # default: iou=0.45 e topk=400\n",
        "\n",
        "        # >>>> Verificar eficácia\n",
        "        # mx.nd.waitall()\n",
        "\n",
        "        # allow the MXNet engine to perform graph optimization for best performance.\n",
        "        self.net.hybridize(static_alloc=True, static_shape=True)\n",
        "\n",
        "        num_of_classes = len(self.classes)\n",
        "        # total number of correct prediction by class\n",
        "        tp = [0] * num_of_classes\n",
        "        # false positives by class\n",
        "        fp = [0] * num_of_classes\n",
        "        # count the number of gt by class\n",
        "        gt_by_class = [0] * num_of_classes\n",
        "        # rec and prec by class\n",
        "        rec_by_class = [0] * num_of_classes\n",
        "        prec_by_class = [0] * num_of_classes\n",
        "        confusion_matrix = np.zeros((num_of_classes, num_of_classes))\n",
        "\n",
        "        for batch in val_data:\n",
        "            batch_size = batch[0].shape[0]\n",
        "            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "\n",
        "            pred_bboxes_list = []\n",
        "            pred_label_list = []\n",
        "            pred_scores_list = []\n",
        "            gt_bboxes_list = []\n",
        "            gt_label_list = []\n",
        "            \n",
        "            for x, y in zip(data, label):\n",
        "                # get prediction results\n",
        "                ids, scores, bboxes = self.net(x)\n",
        "                pred_label_list.append(ids)\n",
        "                pred_scores_list.append(scores)\n",
        "                # clip to image size\n",
        "                pred_bboxes_list.append(bboxes.clip(0, batch[0].shape[2]))\n",
        "                # split ground truths\n",
        "                gt_label_list.append(y.slice_axis(axis=-1, begin=4, end=5))\n",
        "                gt_bboxes_list.append(y.slice_axis(axis=-1, begin=0, end=4))\n",
        "\n",
        "            # Uncomment the following line if you want to plot the images in each inference to visually  check the tp, fp and fn \n",
        "            # self.show_images(x, pred_label_list, pred_bboxes_list, gt_label_list, gt_bboxes_list)\n",
        "            \n",
        "            # update metric\n",
        "            val_metric.update(pred_bboxes_list, pred_label_list, pred_scores_list, gt_bboxes_list, gt_label_list) #, gt_difficults)\n",
        "            \n",
        "            # Get Micro Averaging (precision and recall by each class) in each batch\n",
        "            for img in range(batch_size):\n",
        "                # count +1 for this class id. It will get the total number of gt by class\n",
        "                # It is useful when considering unbalanced datasets\n",
        "                for gt_idx in gt_label_list[0][img]:\n",
        "                    index = int(gt_idx.asnumpy()[0])\n",
        "                    gt_by_class[index] += 1\n",
        "            \n",
        "                for (pred_label, pred_bbox) in zip(pred_label_list[0][img], list(pred_bboxes_list[0][img])):\n",
        "                    pred_label = int(pred_label.asnumpy()[0])\n",
        "                    pred_bbox = pred_bbox.asnumpy()\n",
        "                    pred_bbox = np.expand_dims(pred_bbox, axis=0)\n",
        "                    match = 0\n",
        "                    for (gt_bbox_label, gt_bbox_coordinates) in zip(gt_label_list[0][img], list(gt_bboxes_list[0][img])):\n",
        "                        gt_bbox_coord = gt_bbox_coordinates.asnumpy()\n",
        "                        gt_bbox_coord = np.expand_dims(gt_bbox_coord, axis=0)\n",
        "                        gt_bbox_label = int(gt_bbox_label.asnumpy()[0])\n",
        "                        iou = bbox_iou(pred_bbox, gt_bbox_coord)\n",
        "                        \n",
        "                        # Correct inference\n",
        "                        if iou > validation_threshold and pred_label == gt_bbox_label:\n",
        "                            confusion_matrix[gt_bbox_label][pred_label] += 1\n",
        "                            tp[gt_bbox_label] += 1 # Correct classification\n",
        "                            match = 1\n",
        "                        # Incorrect inference - missed the correct class but put the bounding box in other class\n",
        "                        elif iou > validation_threshold:\n",
        "                            confusion_matrix[gt_bbox_label][pred_label] += 1\n",
        "                            fp[pred_label] += 1\n",
        "                            match = 1\n",
        "                        \n",
        "                    if not match:\n",
        "                        fp[pred_label] += 1\n",
        "                                \n",
        "        # calculate the Recall and Precision by class\n",
        "        tp = np.array(tp) # we can also sum the matrix diagonal\n",
        "        fp = np.array(fp)\n",
        "        \n",
        "        fp_sum = sum(fp)\n",
        "        tp_sum = sum(tp)\n",
        "\n",
        "        # rec and prec according to the micro averaging\n",
        "        for i, (gt_value, tp_value) in enumerate(zip(gt_by_class, tp)):\n",
        "            rec_by_class[i] += tp_value/gt_value\n",
        "            # If an element of fp + tp is 0,\n",
        "            # the corresponding element of prec[l] is nan.\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                prec_by_class[i] += tp_value/(tp_value+fp[i])\n",
        "\n",
        "        return val_metric.get(), rec_by_class, prec_by_class\n",
        "\n",
        "    def create_optimizer(self):\n",
        "        optimizer = self.optimizer\n",
        "        momentum = self.momentum\n",
        "        wd = self.weight_decay\n",
        "        lr = self.learning_rate\n",
        "        beta1 = self.beta1\n",
        "        beta2 = self.beta2\n",
        "        epsilon = self.epsilon\n",
        "        \n",
        "        if optimizer.lower() == 'sgd':\n",
        "            # wd: The weight decay (or L2 regularization) coefficient.\n",
        "            self.trainer = gluon.Trainer(self.net.collect_params(), optimizer,\n",
        "                                    {'learning_rate': lr, 'wd': wd, 'momentum': momentum})\n",
        "        elif optimizer.lower() == 'adam':\n",
        "            self.trainer = gluon.Trainer(self.net.collect_params(), optimizer,\n",
        "                                    {'learning_rate': lr, 'beta1': beta1, 'beta2': beta2, \n",
        "                                     'epsilon': epsilon})\n",
        "\n",
        "    def validate_main(self, epoch):\n",
        "        # consider reduce the frequency of validation to save time\n",
        "        (map_name, mean_ap), rec_by_class, prec_by_class = self.validate()\n",
        "        val_msg = '\\n'.join(['{}={}'.format(k, v) for k, v in zip(map_name, mean_ap)])\n",
        "        \n",
        "        for i, class_name in enumerate(self.classes):\n",
        "            experiment.log_metric('rec_by_class_val_' + class_name, epoch, rec_by_class[i])\n",
        "            experiment.log_metric('prec_by_class_val_' + class_name, epoch, prec_by_class[i])\n",
        "        \n",
        "        for k, v in zip(map_name, mean_ap):\n",
        "            experiment.log_metric('map_' + k, epoch, v)\n",
        "        \n",
        "        print('[Epoch {}] Validation: \\n{}'.format(epoch, val_msg))\n",
        "        current_map = float(mean_ap[-1])\n",
        "        \n",
        "        self.save_params(current_map, epoch)    \n",
        "\n",
        "    def ssd_train(self):\n",
        "        \"\"\"Training pipeline\"\"\"\n",
        "        ctx = self.ctx\n",
        "        train_data = self.train_loader\n",
        "        start_epoch = self.start_epoch\n",
        "        epochs = self.epochs\n",
        "        # experiment = self.experiment\n",
        "        trainer = self.trainer\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
        "        ce_metric = mx.metric.Loss('CrossEntropy')\n",
        "        smoothl1_metric = mx.metric.Loss('SmoothL1')\n",
        "\n",
        "        if optimizer =='sgd':\n",
        "            # lr decay policy\n",
        "            lr_decay = float(lr_decay)\n",
        "            lr_steps = sorted([float(ls) for ls in lr_decay_epoch.split(',') if ls.strip()]) \n",
        "\n",
        "        print('Start training from [Epoch {}]'.format(start_epoch))\n",
        "        start_train_time = time.time()\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            start_epoch_time = time.time()\n",
        "            experiment.log_metric('learning_rate', epoch, trainer.learning_rate)\n",
        "\n",
        "            if optimizer == 'sgd':\n",
        "              while lr_steps and epoch >= lr_steps[0]:\n",
        "                  new_lr = trainer.learning_rate * lr_decay\n",
        "                  lr_steps.pop(0) # removes the first element in the list\n",
        "                  trainer.set_learning_rate(new_lr) # Set a new learning rate\n",
        "                  print(\"[Epoch {}] Set learning rate to {}\".format(epoch, new_lr))\n",
        "                \n",
        "            ce_metric.reset() # Resets the internal evaluation result to initial state.\n",
        "            smoothl1_metric.reset() # Resets the internal evaluation result to initial state.\n",
        "\n",
        "            tic = time.time() # each epoch time in seconds\n",
        "            btic = time.time() # each batch time interval in seconds\n",
        "                \n",
        "            # Activates or deactivates HybridBlocks recursively. it speeds up the training process\n",
        "            self.net.hybridize(static_alloc=True, static_shape=True)\n",
        "                \n",
        "            for i, batch in enumerate(train_data):\n",
        "                # Wait for completion of previous iteration to\n",
        "                # avoid unnecessary memory allocation\n",
        "                # nd.waitall()\n",
        "\n",
        "                batch_size = batch[0].shape[0]\n",
        "                data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
        "                cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
        "                box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
        "\n",
        "                with autograd.record():\n",
        "                    cls_preds = []\n",
        "                    box_preds = []\n",
        "                    for x in data:\n",
        "                        cls_pred, box_pred, _ = self.net(x)\n",
        "                        cls_preds.append(cls_pred)\n",
        "                        box_preds.append(box_pred)\n",
        "                        \n",
        "                    sum_loss, cls_loss, box_loss = mbox_loss(\n",
        "                        cls_preds, box_preds, cls_targets, box_targets)\n",
        "                        \n",
        "                    # with amp.scale_loss(sum_loss, trainer) as scaled_loss:\n",
        "                        # autograd.backward(scaled_loss)\n",
        "                    autograd.backward(sum_loss)\n",
        "                    \n",
        "                # since we have already normalized the loss, we don't want to normalize\n",
        "                # by batch-size anymore\n",
        "                trainer.step(1)\n",
        "                ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
        "                smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
        "\n",
        "                name1, loss1 = ce_metric.get()\n",
        "                name2, loss2 = smoothl1_metric.get()\n",
        "                print('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
        "                    epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
        "                btic = time.time()\n",
        "\n",
        "            # log the epoch info\n",
        "            name1, loss1 = ce_metric.get()\n",
        "            name2, loss2 = smoothl1_metric.get()\n",
        "            experiment.log_metric('cross_entropy_training_loss', epoch, loss1)\n",
        "            experiment.log_metric('smooth_l1_training_loss', epoch, loss2) \n",
        "            experiment.log_metric('train_sum_loss', epoch, loss1 + loss2) \n",
        "\n",
        "            print('[Epoch {}] - Time (min): {:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
        "                epoch, (time.time()-tic)/60, name1, loss1, name2, loss2))\n",
        "\n",
        "            # log SSD LOSS            \n",
        "            val_name1, val_loss1, val_name2, val_loss2 = self.val_loss()\n",
        "            current_val_loss = val_loss1 + val_loss2\n",
        "            experiment.log_metric('cross_entropy_validation_loss', epoch, val_loss1)\n",
        "            experiment.log_metric('smooth_l1_validation_loss', epoch, val_loss2) \n",
        "            experiment.log_metric('validation_sum_loss', epoch, current_val_loss) \n",
        "\n",
        "            self.validate_main(epoch)\n",
        "        \n",
        "        # Displays the total time of the training\n",
        "        print('Train time {:.3f}'.format(time.time() - start_train_time))\n",
        "    \n",
        "    def yolo_train(self):\n",
        "        \"\"\"Training pipeline\"\"\"\n",
        "        ctx = self.ctx\n",
        "        train_data = self.train_loader\n",
        "        start_epoch = self.start_epoch\n",
        "        epochs = self.epochs\n",
        "        # experiment = self.experiment\n",
        "        trainer = self.trainer\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        # metrics\n",
        "        obj_metrics = mx.metric.Loss('ObjLoss')\n",
        "        center_metrics = mx.metric.Loss('BoxCenterLoss')\n",
        "        scale_metrics = mx.metric.Loss('BoxScaleLoss')\n",
        "        cls_metrics = mx.metric.Loss('ClassLoss')\n",
        "\n",
        "        if optimizer =='sgd':\n",
        "            # lr decay policy\n",
        "            lr_decay = float(lr_decay)\n",
        "            lr_steps = sorted([float(ls) for ls in lr_decay_epoch.split(',') if ls.strip()]) \n",
        "\n",
        "        print('Start training from [Epoch {}]'.format(start_epoch))\n",
        "        start_train_time = time.time()\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            experiment.log_metric('learning_rate', epoch, trainer.learning_rate)\n",
        "\n",
        "            start_epoch_time = time.time()\n",
        "            tic = time.time() # each epoch time in seconds\n",
        "            btic = time.time() # each batch time interval in seconds\n",
        "            mx.nd.waitall()\n",
        "            self.net.hybridize()\n",
        "            for i, batch in enumerate(train_data):\n",
        "                data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
        "                # objectness, center_targets, scale_targets, weights, class_targets\n",
        "                fixed_targets = [gluon.utils.split_and_load(batch[it], ctx_list=ctx, batch_axis=0) for it in range(1, 6)]\n",
        "                gt_boxes = gluon.utils.split_and_load(batch[6], ctx_list=ctx, batch_axis=0)\n",
        "                sum_loss = []\n",
        "                obj_losses = []\n",
        "                center_losses = []\n",
        "                scale_losses = []\n",
        "                cls_losses = []\n",
        "                with autograd.record():\n",
        "                    for ix, x in enumerate(data):\n",
        "                        obj_loss, center_loss, scale_loss, cls_loss = self.net(x, gt_boxes[ix], *[ft[ix] for ft in fixed_targets])\n",
        "                        sum_loss.append(obj_loss + center_loss + scale_loss + cls_loss)\n",
        "                        obj_losses.append(obj_loss)\n",
        "                        center_losses.append(center_loss)\n",
        "                        scale_losses.append(scale_loss)\n",
        "                        cls_losses.append(cls_loss)\n",
        "                    # if args.amp:\n",
        "                    # with amp.scale_loss(sum_loss, trainer) as scaled_loss:\n",
        "                    #         autograd.backward(scaled_loss)\n",
        "                    # else:\n",
        "                    autograd.backward(sum_loss)\n",
        "                trainer.step(self.batch_size)\n",
        "                obj_metrics.update(0, obj_losses)\n",
        "                center_metrics.update(0, center_losses)\n",
        "                scale_metrics.update(0, scale_losses)\n",
        "                cls_metrics.update(0, cls_losses)\n",
        "                # if self.log_interval and not (i + 1) % self.log_interval:\n",
        "                name1, loss1 = obj_metrics.get()\n",
        "                name2, loss2 = center_metrics.get()\n",
        "                name3, loss3 = scale_metrics.get()\n",
        "                name4, loss4 = cls_metrics.get()\n",
        "                print('[Epoch {}][Batch {}], LR: {:.2E}, Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
        "                    epoch, i, trainer.learning_rate, self.batch_size/(time.time()-btic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
        "                btic = time.time()\n",
        "\n",
        "\n",
        "            name1, loss1 = obj_metrics.get()\n",
        "            name2, loss2 = center_metrics.get()\n",
        "            name3, loss3 = scale_metrics.get()\n",
        "            name4, loss4 = cls_metrics.get()\n",
        "            experiment.log_metric('Obj_metrics', epoch, loss1)\n",
        "            experiment.log_metric('Center_metrics', epoch, loss2)\n",
        "            experiment.log_metric('Scale_metrics', epoch, loss3)\n",
        "            experiment.log_metric('cls_metrics', epoch, loss4)\n",
        "            print('[Epoch {}] Training cost: {:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
        "                epoch, (time.time()-tic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
        "\n",
        "            self.validate_main(epoch)\n",
        "        \n",
        "        # Displays the total time of the training\n",
        "        print('Train time {:.3f}'.format(time.time() - start_train_time))\n",
        "\n",
        "    def faster_train(self):\n",
        "        print(\"Train start\")\n",
        "        \"\"\"Training pipeline\"\"\"\n",
        "        self.kv_store = 'device' if (False and 'nccl' in self.kv_store) else self.kv_store #if (self.amp and 'nccl' in self.kv_store) else self.kv_store\n",
        "        kv = mx.kvstore.create(self.kv_store)\n",
        "        self.net.collect_params().setattr('grad_req', 'null')\n",
        "        self.net.collect_train_params().setattr('grad_req', 'write')\n",
        "        optimizer_params = {'learning_rate': self.lr, 'wd': self.wd, 'momentum': self.momentum}\n",
        "        self.create_optimizer(kv)\n",
        "        trainer = self.trainer\n",
        "        lr_decay = float(self.lr_decay)\n",
        "        lr_steps = sorted([float(ls) for ls in self.lr_decay_epoch.split(',') if ls.strip()])\n",
        "        lr_warmup = float(self.lr_warmup)  # avoid int division\n",
        "\n",
        "        # TODO(zhreshold) losses?\n",
        "        rpn_cls_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
        "        rpn_box_loss = mx.gluon.loss.HuberLoss(rho=self.rpn_smoothl1_rho)  # == smoothl1\n",
        "        rcnn_cls_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "        rcnn_box_loss = mx.gluon.loss.HuberLoss(rho=self.rcnn_smoothl1_rho)  # == smoothl1\n",
        "        metrics = [mx.metric.Loss('RPN_Conf'),\n",
        "                   mx.metric.Loss('RPN_SmoothL1'),\n",
        "                   mx.metric.Loss('RCNN_CrossEntropy'),\n",
        "                   mx.metric.Loss('RCNN_SmoothL1'), ]\n",
        "\n",
        "        rpn_acc_metric = RPNAccMetric()\n",
        "        rpn_bbox_metric = RPNL1LossMetric()\n",
        "        rcnn_acc_metric = RCNNAccMetric()\n",
        "        rcnn_bbox_metric = RCNNL1LossMetric()\n",
        "        metrics2 = [rpn_acc_metric, rpn_bbox_metric, rcnn_acc_metric, rcnn_bbox_metric]\n",
        "\n",
        "        if self.custom_model:\n",
        "            print('Custom model enabled. Expert Only!! Currently non-FPN model is not supported!!'\n",
        "                        ' Default setting is for MS-COCO.')#logger.info\n",
        "        #logger.info(args)\n",
        "\n",
        "        print('Start training from [Epoch {}]'.format(self.start_epoch))#logger.info\n",
        "        best_map = [0]\n",
        "        for epoch in range(self.start_epoch, self.epochs):\n",
        "\n",
        "            #self.experiment.log_metric('learning_rate', epoch, trainer.learning_rate)\n",
        "\n",
        "            rcnn_task = ForwardBackwardTask(self.net, trainer, rpn_cls_loss, rpn_box_loss, rcnn_cls_loss,\n",
        "                                            rcnn_box_loss, mix_ratio=1.0, amp_enabled=self.amp)\n",
        "            executor = Parallel(self.executor_threads, rcnn_task) if not self.horovod else None\n",
        "            mix_ratio = 1.0\n",
        "            if not self.disable_hybridization:\n",
        "                self.hybridize(static_alloc=self.static_alloc)\n",
        "            if self.mixup:\n",
        "                # TODO(zhreshold) only support evenly mixup now, target generator needs to be modified otherwise\n",
        "                self.train_data._dataset._data.set_mixup(np.random.uniform, 0.5, 0.5)\n",
        "                mix_ratio = 0.5\n",
        "                if epoch >= self.epochs - self.no_mixup_epochs:\n",
        "                    self.train_data._dataset._data.set_mixup(None)\n",
        "                    mix_ratio = 1.0\n",
        "\n",
        "            experiment.log_metric('learning_rate', epoch, trainer.learning_rate)\n",
        "            if self.optimizer == 'sgd':\n",
        "              while lr_steps and epoch >= lr_steps[0]:\n",
        "                  new_lr = trainer.learning_rate * lr_decay\n",
        "                  lr_steps.pop(0)\n",
        "                  trainer.set_learning_rate(new_lr)\n",
        "                  print(\"[Epoch {}] Set learning rate to {}\".format(epoch, new_lr))#logger.info\n",
        "            for metric in metrics:\n",
        "                metric.reset()\n",
        "            tic = time.time()\n",
        "            btic = time.time()\n",
        "            base_lr = trainer.learning_rate\n",
        "            rcnn_task.mix_ratio = mix_ratio\n",
        "            for i, batch in enumerate(self.train_data):\n",
        "                if epoch == 0 and i <= lr_warmup:\n",
        "                    # adjust based on real percentage\n",
        "                    new_lr = base_lr * self.get_lr_at_iter(i / lr_warmup, self.lr_warmup_factor)\n",
        "                    if new_lr != trainer.learning_rate:\n",
        "                        if i % self.log_interval == 0:\n",
        "                            print(\n",
        "                                '[Epoch 0 Iteration {}] Set learning rate to {}'.format(i, new_lr))#logger.info\n",
        "                        trainer.set_learning_rate(new_lr)\n",
        "                batch = self.split_and_load(batch, ctx_list=self.ctx)\n",
        "                metric_losses = [[] for _ in metrics]\n",
        "                add_losses = [[] for _ in metrics2]\n",
        "                if executor is not None:\n",
        "                    for data in zip(*batch):\n",
        "                        executor.put(data)\n",
        "                for j in range(len(self.ctx)):\n",
        "                    if executor is not None:\n",
        "                        result = executor.get()\n",
        "                    else:\n",
        "                        result = rcnn_task.forward_backward(list(zip(*batch))[0])\n",
        "\n",
        "                    for k in range(len(metric_losses)):\n",
        "                        metric_losses[k].append(result[k])\n",
        "                    for k in range(len(add_losses)):\n",
        "                        add_losses[k].append(result[len(metric_losses) + k])\n",
        "                for metric, record in zip(metrics, metric_losses):\n",
        "                    metric.update(0, record)\n",
        "                for metric, records in zip(metrics2, add_losses):\n",
        "                    for pred in records:\n",
        "                        metric.update(pred[0], pred[1])\n",
        "                trainer.step(self.batch_size)\n",
        "\n",
        "                # update metrics        #  (not self.horovod or hvd.rank() == 0) and  _k_\n",
        "                if self.log_interval \\\n",
        "                        and not (i + 1) % self.log_interval:\n",
        "                    msg = ','.join(\n",
        "                        ['{}={:.3f}'.format(*metric.get()) for metric in metrics + metrics2])\n",
        "                    print('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}'.format(\n",
        "                        epoch, i, self.log_interval * self.batch_size / (time.time() - btic), msg)) #logger.info\n",
        "                    btic = time.time()\n",
        "\n",
        "            # if (not self.horovod) or hvd.rank() == 0:\n",
        "            msg = ','.join(['{}={:.3f}'.format(*metric.get()) for metric in metrics])\n",
        "            print('[Epoch {}] Training cost: {:.3f}, {}'.format(\n",
        "                epoch, (time.time() - tic), msg)) #logger.info\n",
        "            \n",
        "            self.validate_main(epoch)#map_name, mean_ap = self.validate(self.net, self.val_data, self.ctx, self.val_metric)\n",
        " \n",
        "        print(\"Traning... OK\")    \n",
        "            \n",
        "    def train(self):\n",
        "        lr_decay = self.lr_decay\n",
        "        lr_decay_epoch = self.lr_decay_epoch        \n",
        "        optimizer = self.optimizer\n",
        "        network = self.network\n",
        "\n",
        "        # Gluon-CV requires you to create and load the parameters of your model first on \n",
        "        # the CPU - so specify ctx=None - and when all that is done you move the \n",
        "        # whole model on the GPU with:\n",
        "        self.net.collect_params().reset_ctx(self.ctx)\n",
        "\n",
        "        # # First create the trainer. Obs: you should reset_ctx before creating the optimizer\n",
        "        # self.create_optimizer()\n",
        "\n",
        "        # speeds up the training process\n",
        "        # Check: https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/amp.html\n",
        "        # trainer = self.trainer\n",
        "        # amp.init_trainer(trainer)\n",
        "\n",
        "        if self.net_type == 'ssd':\n",
        "          # First create the trainer. Obs: you should reset_ctx before creating the optimizer\n",
        "            self.create_optimizer()\n",
        "            self.ssd_train()\n",
        "        elif self.net_type == 'yolo':\n",
        "          # First create the trainer. Obs: you should reset_ctx before creating the optimizer\n",
        "            self.create_optimizer()\n",
        "            self.yolo_train()\n",
        "        elif self.net_type =='faster':\n",
        "            self.faster_train()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtDYBNbDuOw_",
        "colab_type": "text"
      },
      "source": [
        "# Faster RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEhS8oQWs9wE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"\"\"Train Faster-RCNN end to end.\"\"\"\n",
        "# import argparse\n",
        "# import os\n",
        "\n",
        "# disable autotune\n",
        "\n",
        "\n",
        "# try:\n",
        "#     import horovod.mxnet as hvd\n",
        "# except ImportError:\n",
        "#     hvd = None\n",
        "\n",
        "\n",
        "class Faster_rcnn(training_network):\n",
        "    def __init__(self, model='faster_rcnn_resnet50_v1b_voc' ,network = 'resnet50_v1b', dataset = 'voc',num_workers = 4,batch_size = 1,gpu = 0,epochs = 20,\n",
        "                 resume = '',start_epoch = 0,lr = 0.001,lr_decay = 0.1,lr_decay_epoch= '14,20',lr_warmup = -1, beta1=0.9, beta2=0.999, epsilon=1e-08,\n",
        "                 lr_warmup_factor = 1. / 3.,momentum = 0.9, validation_threshold=0.5, nms_threshold=0.5,wd = 5e-4,log_interval = 1,save_interval = 1, optimizer = 'sgd',\n",
        "                 val_interval = 1,exp = False,train_file = 'train.rec',val_file = 'val.rec',ctx = 'gpu',save_prefix = 'faster_cnn_model_', x_id=None):\n",
        "        self.x_id = x_id\n",
        "        self.old_save = None\n",
        "        self.net_type = 'faster'\n",
        "        self.batch_size=batch_size #\n",
        "        self.num_workers=num_workers#\n",
        "        self.learning_rate = lr#\n",
        "        self.lr = lr#\n",
        "        self.weight_decay = wd#\n",
        "        self.wd = wd#\n",
        "        self.momentum = momentum#\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_decay = lr_decay#\n",
        "        self.lr_decay_epoch = lr_decay_epoch#\n",
        "        self.val_interval = val_interval#\n",
        "        self.start_epoch = start_epoch#\n",
        "        self.epochs = epochs#\n",
        "        self.log_interval = log_interval#\n",
        "        self.save_interval = save_interval#\n",
        "        self.resume = resume#\n",
        "        self.validation_threshold = validation_threshold\n",
        "        self.nms_threshold = nms_threshold\n",
        "        self.network = network#\n",
        "        self.dataset = dataset#\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.epsilon=epsilon\n",
        "        self.trainer = None\n",
        "        self.model = model.lower()\n",
        "        \n",
        "        self.gpu = gpu#\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        self.lr_warmup = lr_warmup#\n",
        "        self.lr_warmup_factor = lr_warmup_factor#\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        self.mixup = False\n",
        "        self.no_mixup_epochs = 20\n",
        "        self.norm_layer = None\n",
        "        self.rpn_smoothl1_rho = 1. / 9.\n",
        "        self.rcnn_smoothl1_rho = 1\n",
        "        self.use_fpn = False\n",
        "        self.disable_hybridization = True\n",
        "        self.static_alloc = True\n",
        "        self.amp = False\n",
        "        self.horovod = False\n",
        "        self.executor_threads = 2\n",
        "        self.kv_store = 'nccl'\n",
        "        self.experiment = exp\n",
        "\n",
        "        self.save_prefix = save_prefix\n",
        "        self.train_file = train_file\n",
        "        self.val_file = val_file\n",
        "        self.validation_threshold = 0.5\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.val_metric = None\n",
        "        self.classes = ['bar_clamp', 'gear_box', 'vase', 'part_1', 'part_3', 'nozzle', 'pawn', 'turbine_housing']\n",
        "        #Configuração da rede\n",
        "        self.custom_model = False\n",
        "        self.no_pretrained_base = True\n",
        "        self.num_fpn_filters = 256\n",
        "        self.num_box_head_conv = 4\n",
        "        self.num_box_head_conv_filters = 256\n",
        "        self.num_box_head_dense_filters = 1024\n",
        "        self.image_short = 800\n",
        "        self.image_max_size = 1033\n",
        "        self.nms_thresh = 0.5\n",
        "        self.nms_topk = -1\n",
        "        self.post_nms = -1\n",
        "        self.roi_mode = 'align'\n",
        "        self.roi_size = '7,7'\n",
        "        self.strides = '4,8,16,32,64'\n",
        "        self.clip = 4.14\n",
        "        self.rpn_channel = 256\n",
        "        self.anchor_base_size = 16\n",
        "        self.anchor_aspect_ratio = '0.5,1,2'\n",
        "        self.anchor_scales = '2,4,8,16,32'\n",
        "        self.anchor_alloc_size = '384,384'\n",
        "        self.rpn_ms_thresh = '0.7'\n",
        "        self.rpn_train_pre_nms = 12000\n",
        "        self.rpn_train_post_nms = 2000\n",
        "        self.rpn_test_pre_nms = 6000\n",
        "        self.rpn_test_post_nms = 1000\n",
        "        self.rpn_min_size= 1\n",
        "        self.rcnn_num_sample = 512\n",
        "        self.rcnn_pos_iou_thresh = 0.5\n",
        "        self.rcnn_pos_ratio = 0.25\n",
        "        self.max_num_gt = 100\n",
        "        self.best_map=0\n",
        "\n",
        "        if ctx == 'cpu':\n",
        "            self.ctx = [mx.cpu()]\n",
        "        elif ctx == 'gpu':\n",
        "            self.ctx = [mx.gpu(self.gpu)]\n",
        "        else:\n",
        "            raise ValueError('Invalid context.')\n",
        "\n",
        "        # fix seed for mxnet, numpy and python builtin random generator.\n",
        "        gutils.random.seed(233)\n",
        "\n",
        "        self.net = None\n",
        "\n",
        "        self.get_dataset()\n",
        "        self.create_network()\n",
        "        self.data_loader()\n",
        "\n",
        "    # def get_dataset(self):\n",
        "\n",
        "    #     self.train_dataset = gdata.RecordFileDetection(self.train_file)\n",
        "    #     self.val_dataset = gdata.RecordFileDetection(self.val_file)\n",
        "    #     self.val_metric = VOC07MApMetric(iou_thresh=0.5, class_names=self.classes)\n",
        "    #     print(\"Dataset... OK\")\n",
        "\n",
        "    def create_network(self):\n",
        "        kwargs = {}\n",
        "        module_list = []\n",
        "        if self.use_fpn:\n",
        "            module_list.append('fpn')\n",
        "        if self.norm_layer is not None:\n",
        "            module_list.append(self.norm_layer)\n",
        "            if self.norm_layer == 'syncbn':\n",
        "                kwargs['num_devices'] = len(self.ctx)\n",
        "\n",
        "        num_gpus =len(self.ctx) # hvd.size() if self.horovod else __k__\n",
        "        net_name = '_'.join(('faster_rcnn', *module_list, self.network, self.dataset))\n",
        "        if self.custom_model:\n",
        "            self.use_fpn = True\n",
        "            net_name = '_'.join(('custom_faster_rcnn_fpn', self.network, self.dataset))\n",
        "            if self.norm_layer == 'syncbn':\n",
        "                norm_layer = gluon.contrib.nn.SyncBatchNorm\n",
        "                norm_kwargs = {'num_devices': len(self.ctx)}\n",
        "                sym_norm_layer = mx.sym.contrib.SyncBatchNorm\n",
        "                sym_norm_kwargs = {'ndev': len(self.ctx)}\n",
        "            elif self.norm_layer == 'gn':\n",
        "                norm_layer = gluon.nn.GroupNorm\n",
        "                norm_kwargs = {'groups': 8}\n",
        "                sym_norm_layer = mx.sym.GroupNorm\n",
        "                sym_norm_kwargs = {'groups': 8}\n",
        "            else:\n",
        "                norm_layer = gluon.nn.BatchNorm\n",
        "                norm_kwargs = None\n",
        "                sym_norm_layer = None\n",
        "                sym_norm_kwargs = None\n",
        "            classes = self.classes\n",
        "            self.net = get_model('custom_faster_rcnn_fpn', classes=classes, transfer=None,\n",
        "                            dataset=self.dataset, pretrained_base=not self.no_pretrained_base,\n",
        "                            base_network_name=self.network, norm_layer=norm_layer,\n",
        "                            norm_kwargs=norm_kwargs, sym_norm_kwargs=sym_norm_kwargs,\n",
        "                            num_fpn_filters=self.num_fpn_filters,\n",
        "                            num_box_head_conv= self.num_box_head_conv,\n",
        "                            num_box_head_conv_filters=self.num_box_head_conv_filters,\n",
        "                            num_box_head_dense_filters=self.num_box_head_dense_filters,\n",
        "                            short=self.image_short, max_size=self.image_max_size, min_stage=2,\n",
        "                            max_stage=6, nms_thresh=self.nms_thresh, nms_topk=self.nms_topk,\n",
        "                            post_nms=self.post_nms, roi_mode=self.roi_mode, roi_size=self.roi_size,\n",
        "                            strides=self.strides, clip=self.clip, rpn_channel=self.rpn_channel,\n",
        "                            base_size=self.anchor_base_size, scales=self.anchor_scales,\n",
        "                            ratios=self.anchor_aspect_ratio, alloc_size=self.anchor_alloc_size,\n",
        "                            rpn_nms_thresh=self.rpn_nms_thresh,\n",
        "                            rpn_train_pre_nms=self.rpn_train_pre_nms,\n",
        "                            rpn_train_post_nms=self.rpn_train_post_nms,\n",
        "                            rpn_test_pre_nms=self.rpn_test_pre_nms,\n",
        "                            rpn_test_post_nms=self.rpn_test_post_nms, rpn_min_size=self.rpn_min_size,\n",
        "                            per_device_batch_size=self.batch_size // num_gpus,\n",
        "                            num_sample=self.rcnn_num_samples, pos_iou_thresh=self.rcnn_pos_iou_thresh,\n",
        "                            pos_ratio=self.rcnn_pos_ratio, max_num_gt=self.max_num_gt)\n",
        "        else:\n",
        "            self.net = get_model(net_name, pretrained_base=True,\n",
        "                            per_device_batch_size=self.batch_size // num_gpus, **kwargs)\n",
        "        # self.save_prefix += net_name\n",
        "        if self.resume.strip():\n",
        "            self.net.load_parameters(self.resume.strip())\n",
        "        else:\n",
        "            for param in self.net.collect_params().values():\n",
        "                if param._data is not None:\n",
        "                    continue\n",
        "                param.initialize()\n",
        "        self.net.collect_params().reset_ctx(self.ctx)\n",
        "        print(\"Network... OK\")\n",
        "\n",
        "    def data_loader(self):\n",
        "        \"\"\"Get dataloader.\"\"\"\n",
        "        train_transform = FasterRCNNDefaultTrainTransform\n",
        "        val_transform = FasterRCNNDefaultValTransform\n",
        "        train_bfn = FasterRCNNTrainBatchify(self.net, len(self.ctx))\n",
        "        if hasattr(self.train_dataset, 'get_im_aspect_ratio'):\n",
        "            im_aspect_ratio = self.train_dataset.get_im_aspect_ratio()\n",
        "        else:\n",
        "            im_aspect_ratio = [1.] * len(self.train_dataset)\n",
        "        train_sampler = \\\n",
        "            gcv.nn.sampler.SplitSortedBucketSampler(im_aspect_ratio, self.batch_size,\n",
        "                                                    num_parts=1, #hvd.size() if self.horovod else _k_\n",
        "                                                    part_index=0, #hvd.rank() if self.horovod else _k_ \n",
        "                                                    shuffle=True)\n",
        "        self.train_data = mx.gluon.data.DataLoader(self.train_dataset.transform(\n",
        "            train_transform(self.net.short, self.net.max_size, self.net, ashape=self.net.ashape, multi_stage=self.use_fpn)),\n",
        "            batch_sampler=train_sampler, batchify_fn=train_bfn, num_workers=self.num_workers)\n",
        "        val_bfn = Tuple(*[Append() for _ in range(3)])\n",
        "        short = self.net.short[-1] if isinstance(self.net.short, (tuple, list)) else self.net.short\n",
        "        # validation use 1 sample per device\n",
        "        self.val_data = mx.gluon.data.DataLoader(\n",
        "            self.val_dataset.transform(val_transform(short, self.net.max_size)), len(self.ctx), False,\n",
        "            batchify_fn=val_bfn, last_batch='keep', num_workers=self.num_workers)\n",
        "        print(\"Dataloader... OK\")\n",
        "\n",
        "    def get_lr_at_iter(self,alpha, lr_warmup_factor=1. / 3.):\n",
        "        return lr_warmup_factor * (1 - alpha) + alpha\n",
        "\n",
        "    def split_and_load(self,batch, ctx_list):\n",
        "        \"\"\"Split data to 1 batch each device.\"\"\"\n",
        "        new_batch = []\n",
        "        for i, data in enumerate(batch):\n",
        "            if isinstance(data, (list, tuple)):\n",
        "                new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)]\n",
        "            else:\n",
        "                new_data = [data.as_in_context(ctx_list[0])]\n",
        "            new_batch.append(new_data)\n",
        "        return new_batch\n",
        "    \n",
        "\n",
        "#FASTER  ##,net, val_data, ctx, val_metric\n",
        "    def validate(self):\n",
        "        \"\"\"Test on validation dataset.\"\"\"\n",
        "        val_data = self.val_data\n",
        "        ctx = self.ctx\n",
        "        val_metric = self.val_metric\n",
        "        nms_threshold = self.nms_threshold\n",
        "        validation_threshold = self.validation_threshold\n",
        "\n",
        "        clipper = gcv.nn.bbox.BBoxClipToImage()\n",
        "        val_metric.reset()\n",
        "        # if not self.disable_hybridization:\n",
        "            # input format is differnet than training, thus rehybridization is needed.\n",
        "        # self.net.hybridize(static_alloc=self.static_alloc)\n",
        "        for batch in val_data:\n",
        "            batch = self.split_and_load(batch, ctx_list=ctx)\n",
        "            pred_bboxes_list = []\n",
        "            pred_label_list = []\n",
        "            pred_scores_list = []\n",
        "            gt_bboxes_list = []\n",
        "            gt_label_list = []\n",
        "            # gt_difficults = []\n",
        "            for x, y, im_scale in zip(*batch):\n",
        "                # get prediction results\n",
        "                ids, scores, bboxes = self.net(x)\n",
        "                pred_label_list.append(ids)\n",
        "                pred_scores_list.append(scores)\n",
        "                # clip to image size\n",
        "                pred_bboxes_list.append(clipper(bboxes, x))\n",
        "                # rescale to original resolution\n",
        "                im_scale = im_scale.reshape((-1)).asscalar()\n",
        "                pred_bboxes_list[-1] *= im_scale\n",
        "                # split ground truths\n",
        "                gt_label_list.append(y.slice_axis(axis=-1, begin=4, end=5))\n",
        "                gt_bboxes_list.append(y.slice_axis(axis=-1, begin=0, end=4))\n",
        "                gt_bboxes_list[-1] *= im_scale\n",
        "                # gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n",
        "\n",
        "            # update metric\n",
        "            for pred_bbox, pred_id, pred_score, gt_bbox, gt_id in zip(pred_bboxes_list, pred_label_list,\n",
        "                                                                            pred_scores_list, gt_bboxes_list,\n",
        "                                                                            gt_label_list):\n",
        "                val_metric.update(pred_bbox, pred_id, pred_score, gt_bbox, gt_id)\n",
        "\n",
        "        num_of_classes = len(self.classes)\n",
        "        # total number of correct prediction by class\n",
        "        tp = [0] * num_of_classes\n",
        "        # false positives by class\n",
        "        fp = [0] * num_of_classes\n",
        "        # count the number of gt by class\n",
        "        gt_by_class = [0] * num_of_classes\n",
        "        # rec and prec by class\n",
        "        rec_by_class = [0] * num_of_classes\n",
        "        prec_by_class = [0] * num_of_classes\n",
        "        confusion_matrix = np.zeros((num_of_classes, num_of_classes))\n",
        "        for img in range(self.batch_size):\n",
        "              # count +1 for this class id. It will get the total number of gt by class\n",
        "              # It is useful when considering unbalanced datasets\n",
        "              for gt_idx in gt_label_list[0][img]:\n",
        "                  index = int(gt_idx.asnumpy()[0])\n",
        "                  gt_by_class[index] += 1\n",
        "          \n",
        "              for (pred_label, pred_bbox) in zip(pred_label_list[0][img], list(pred_bboxes_list[0][img])):\n",
        "                  pred_label = int(pred_label.asnumpy()[0])\n",
        "                  pred_bbox = pred_bbox.asnumpy()\n",
        "                  pred_bbox = np.expand_dims(pred_bbox, axis=0)\n",
        "                  match = 0\n",
        "                  for (gt_bbox_label, gt_bbox_coordinates) in zip(gt_label_list[0][img], list(gt_bboxes_list[0][img])):\n",
        "                      gt_bbox_coord = gt_bbox_coordinates.asnumpy()\n",
        "                      gt_bbox_coord = np.expand_dims(gt_bbox_coord, axis=0)\n",
        "                      gt_bbox_label = int(gt_bbox_label.asnumpy()[0])\n",
        "                      iou = bbox_iou(pred_bbox, gt_bbox_coord)\n",
        "                      \n",
        "                      # Correct inference\n",
        "                      if iou > validation_threshold and pred_label == gt_bbox_label:\n",
        "                          confusion_matrix[gt_bbox_label][pred_label] += 1\n",
        "                          tp[gt_bbox_label] += 1 # Correct classification\n",
        "                          match = 1\n",
        "                      # Incorrect inference - missed the correct class but put the bounding box in other class\n",
        "                      elif iou > validation_threshold:\n",
        "                          confusion_matrix[gt_bbox_label][pred_label] += 1\n",
        "                          fp[pred_label] += 1\n",
        "                          match = 1\n",
        "                      \n",
        "                  if not match:\n",
        "                      fp[pred_label] += 1\n",
        "                              \n",
        "        # calculate the Recall and Precision by class\n",
        "        tp = np.array(tp) # we can also sum the matrix diagonal\n",
        "        fp = np.array(fp)\n",
        "        fp_sum = sum(fp)\n",
        "        tp_sum = sum(tp)\n",
        "\n",
        "        # rec and prec according to the micro averaging\n",
        "        for i, (gt_value, tp_value) in enumerate(zip(gt_by_class, tp)):\n",
        "            rec_by_class[i] += tp_value/gt_value\n",
        "            # If an element of fp + tp is 0,\n",
        "            # the corresponding element of prec[l] is nan.\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                prec_by_class[i] += tp_value/(tp_value+fp[i])\n",
        "\n",
        "        return val_metric.get(), rec_by_class, prec_by_class\n",
        "\n",
        "    def create_optimizer(self, kv):\n",
        "        optimizer = self.optimizer\n",
        "        momentum = self.momentum\n",
        "        wd = self.wd\n",
        "        lr = self.lr\n",
        "        beta1 = self.beta1\n",
        "        beta2 = self.beta2\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        if optimizer == 'sgd':\n",
        "            optimizer_params = {'multi_precision' : True, 'learning_rate': lr, \n",
        "                                          'wd': wd, 'momentum': momentum}\n",
        "            # wd: The weight decay (or L2 regularization) coefficient.\n",
        "            \n",
        "        elif optimizer == 'adam':\n",
        "            optimizer_params = {'multi_precision' : True, 'learning_rate': lr, 'beta1': beta1, 'beta2': beta2,\n",
        "                                          'epsilon': epsilon}\n",
        "    \n",
        "        self.trainer = gluon.Trainer(self.net.collect_train_params(), optimizer, optimizer_params\n",
        "                                    , update_on_kvstore=(False if False else None), kvstore=kv) #self.amp"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhm3jG8jtQ4p",
        "colab_type": "text"
      },
      "source": [
        "# Função Main "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi38NtjktQSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34e899d1-ef96-4566-df9b-5814a9441f1f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  try:\n",
        "    neptune.init('caioviturino/IJR2020')\n",
        "    #'yolo3_darknet53_coco'\n",
        "    #'faster_rcnn_resnet50_v1b_voc'\n",
        "    PARAMS = {'model_name': 'yolo3_darknet53_coco', \n",
        "              'ctx': 'gpu',\n",
        "              'lr_decay_epoch': '30,50',\n",
        "              'lr': 1e-3, # 0.0001,\n",
        "              'lr_decay': 0.1,\n",
        "              'batch_size': 16,\n",
        "              'epochs': 60, #80\n",
        "              'optimizer': 'adam', #https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html\n",
        "              'wd': 5e-4, # 0.0005, # sgd parameter\n",
        "              'momentum': 0.9, # 0.9, # sgd parameter\n",
        "              'beta1': 0.9, # 0.9, # adam parameter\n",
        "              'beta2': 0.999, #0.999, # adam parameter\n",
        "              'epsilon': 1e-08, # 1e-08, # adam parameter\n",
        "              'validation_threshold': 0.5,\n",
        "              'nms_threshold': 0.5\n",
        "              }\n",
        "\n",
        "    # create experiment (all parameters are optional)\n",
        "    experiment = neptune.create_experiment(name=PARAMS['model_name'],\n",
        "                                           params=PARAMS,\n",
        "                                           tags=[PARAMS['model_name'], PARAMS['optimizer'], 'kleber'])\n",
        "    \n",
        "    \n",
        "    print('Experiment ID = ' + experiment.id + '\\n Net= '+ PARAMS['model_name'])\n",
        "\n",
        "\n",
        "    ## TODO : Trocar == por *contains*\n",
        "    \n",
        "    if PARAMS['model_name'] == 'faster_rcnn_resnet50_v1b_voc':\n",
        "      train_loc = 'drive/My Drive/UFBA/Doutorado/Ssd_test/Dataset/train_teste_7_300_300.rec'\n",
        "      val_loc = 'drive/My Drive/UFBA/Doutorado/Ssd_test/Dataset/val_teste_7_300_300.rec'\n",
        "      save_loc = 'drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints'\n",
        "      fast = Faster_rcnn(model=PARAMS['model_name'], train_file=train_loc, val_file=val_loc,save_prefix=save_loc, ctx=PARAMS['ctx'], \\\n",
        "                         lr_decay_epoch=PARAMS['lr_decay_epoch'], lr=PARAMS['lr'], \\\n",
        "                         lr_decay=PARAMS['lr_decay'], batch_size=1, epochs=PARAMS['epochs'], \\\n",
        "                         optimizer=PARAMS['optimizer'],  \\\n",
        "                         validation_threshold=PARAMS['validation_threshold'], nms_threshold=PARAMS['nms_threshold'], \\\n",
        "                         beta1=PARAMS['beta1'], beta2=PARAMS['beta2'], epsilon=PARAMS['epsilon'], \\\n",
        "                         wd=PARAMS['wd'], momentum=PARAMS['momentum'], x_id = experiment.id) \n",
        "                         #exp = experiment\n",
        "      # print('\\n'.join(str(a) for a in dir(fast)))\n",
        "      fast.faster_train()\n",
        "      os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'\n",
        "      os.environ['MXNET_GPU_MEM_POOL_TYPE'] = 'Round'\n",
        "      os.environ['MXNET_GPU_MEM_POOL_ROUND_LINEAR_CUTOFF'] = '26'\n",
        "      os.environ['MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN_FWD'] = '999'\n",
        "      os.environ['MXNET_EXEC_BULK_EXEC_MAX_NODE_TRAIN_BWD'] = '25'\n",
        "      os.environ['MXNET_GPU_COPY_NTHREADS'] = '1'\n",
        "      os.environ['MXNET_OPTIMIZER_AGGREGATION_SIZE'] = '54'\n",
        "    else:\n",
        "      train_object = training_network(model=PARAMS['model_name'], ctx=PARAMS['ctx'], \\\n",
        "                                          lr_decay_epoch=PARAMS['lr_decay_epoch'], lr=PARAMS['lr'], \\\n",
        "                                          lr_decay=PARAMS['lr_decay'], batch_size=PARAMS['batch_size'], epochs=PARAMS['epochs'], \\\n",
        "                                          optimizer=PARAMS['optimizer'],  \\\n",
        "                                          validation_threshold=PARAMS['validation_threshold'], nms_threshold=PARAMS['nms_threshold'], \\\n",
        "                                          beta1=PARAMS['beta1'], beta2=PARAMS['beta2'], epsilon=PARAMS['epsilon'], \\\n",
        "                                          wd=PARAMS['wd'], momentum=PARAMS['momentum'], x_id = experiment.id)\n",
        "  # exp=experiment,\n",
        "      train_object.get_dataset()\n",
        "      # train_object.show_summary()\n",
        "      # Loads the dataset according to the batch size and num_workers\n",
        "      train_object.get_dataloader()\n",
        "      # training\n",
        "      train_object.train()\n",
        "  except:\n",
        "    raise\n",
        "  finally:\n",
        "    neptune.stop()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ui.neptune.ai/caioviturino/IJR2020/e/SAN-349\n",
            "Experiment ID = SAN-349\n",
            " Net= yolo3_darknet53_coco\n",
            "Classes:  ['bar_clamp', 'gear_box', 'vase', 'part_1', 'part_3', 'nozzle', 'pawn', 'turbine_housing']\n",
            "Start training from [Epoch 0]\n",
            "[Epoch 0][Batch 0], LR: 1.00E-03, Speed: 3.772 samples/sec, ObjLoss=4142.146, BoxCenterLoss=20.842, BoxScaleLoss=13.236, ClassLoss=41.933\n",
            "[Epoch 0][Batch 1], LR: 1.00E-03, Speed: 14.258 samples/sec, ObjLoss=4006.351, BoxCenterLoss=20.661, BoxScaleLoss=11.806, ClassLoss=39.672\n",
            "[Epoch 0][Batch 2], LR: 1.00E-03, Speed: 14.134 samples/sec, ObjLoss=3817.952, BoxCenterLoss=19.744, BoxScaleLoss=10.715, ClassLoss=36.861\n",
            "[Epoch 0][Batch 3], LR: 1.00E-03, Speed: 13.419 samples/sec, ObjLoss=3632.876, BoxCenterLoss=19.477, BoxScaleLoss=10.019, ClassLoss=35.071\n",
            "[Epoch 0][Batch 4], LR: 1.00E-03, Speed: 14.351 samples/sec, ObjLoss=3456.032, BoxCenterLoss=19.180, BoxScaleLoss=9.182, ClassLoss=33.177\n",
            "[Epoch 0][Batch 5], LR: 1.00E-03, Speed: 14.452 samples/sec, ObjLoss=3291.053, BoxCenterLoss=19.110, BoxScaleLoss=8.701, ClassLoss=32.097\n",
            "[Epoch 0][Batch 6], LR: 1.00E-03, Speed: 14.274 samples/sec, ObjLoss=3138.768, BoxCenterLoss=19.199, BoxScaleLoss=8.361, ClassLoss=31.515\n",
            "[Epoch 0][Batch 7], LR: 1.00E-03, Speed: 12.388 samples/sec, ObjLoss=2998.412, BoxCenterLoss=18.847, BoxScaleLoss=7.914, ClassLoss=30.303\n",
            "[Epoch 0][Batch 8], LR: 1.00E-03, Speed: 11.437 samples/sec, ObjLoss=2868.243, BoxCenterLoss=18.787, BoxScaleLoss=7.646, ClassLoss=29.652\n",
            "[Epoch 0][Batch 9], LR: 1.00E-03, Speed: 11.192 samples/sec, ObjLoss=2747.527, BoxCenterLoss=18.681, BoxScaleLoss=7.424, ClassLoss=29.020\n",
            "[Epoch 0][Batch 10], LR: 1.00E-03, Speed: 11.780 samples/sec, ObjLoss=2635.257, BoxCenterLoss=18.580, BoxScaleLoss=7.154, ClassLoss=28.454\n",
            "[Epoch 0][Batch 11], LR: 1.00E-03, Speed: 10.758 samples/sec, ObjLoss=2531.059, BoxCenterLoss=18.604, BoxScaleLoss=6.966, ClassLoss=28.118\n",
            "[Epoch 0][Batch 12], LR: 1.00E-03, Speed: 13.152 samples/sec, ObjLoss=2434.163, BoxCenterLoss=18.695, BoxScaleLoss=6.863, ClassLoss=27.926\n",
            "[Epoch 0][Batch 13], LR: 1.00E-03, Speed: 11.060 samples/sec, ObjLoss=2343.664, BoxCenterLoss=18.823, BoxScaleLoss=6.741, ClassLoss=27.746\n",
            "[Epoch 0][Batch 14], LR: 1.00E-03, Speed: 9.843 samples/sec, ObjLoss=2259.013, BoxCenterLoss=18.869, BoxScaleLoss=6.695, ClassLoss=27.522\n",
            "[Epoch 0][Batch 15], LR: 1.00E-03, Speed: 13.669 samples/sec, ObjLoss=2179.530, BoxCenterLoss=18.860, BoxScaleLoss=6.618, ClassLoss=27.225\n",
            "[Epoch 0][Batch 16], LR: 1.00E-03, Speed: 10.385 samples/sec, ObjLoss=2105.237, BoxCenterLoss=18.789, BoxScaleLoss=6.519, ClassLoss=26.878\n",
            "[Epoch 0][Batch 17], LR: 1.00E-03, Speed: 14.343 samples/sec, ObjLoss=2035.173, BoxCenterLoss=18.601, BoxScaleLoss=6.382, ClassLoss=26.403\n",
            "[Epoch 0][Batch 18], LR: 1.00E-03, Speed: 10.291 samples/sec, ObjLoss=1969.446, BoxCenterLoss=18.615, BoxScaleLoss=6.308, ClassLoss=26.187\n",
            "[Epoch 0][Batch 19], LR: 1.00E-03, Speed: 14.484 samples/sec, ObjLoss=1907.403, BoxCenterLoss=18.571, BoxScaleLoss=6.242, ClassLoss=25.940\n",
            "[Epoch 0][Batch 20], LR: 1.00E-03, Speed: 12.251 samples/sec, ObjLoss=1848.885, BoxCenterLoss=18.491, BoxScaleLoss=6.152, ClassLoss=25.659\n",
            "[Epoch 0][Batch 21], LR: 1.00E-03, Speed: 14.347 samples/sec, ObjLoss=1793.474, BoxCenterLoss=18.400, BoxScaleLoss=6.048, ClassLoss=25.411\n",
            "[Epoch 0][Batch 22], LR: 1.00E-03, Speed: 9.016 samples/sec, ObjLoss=1741.086, BoxCenterLoss=18.413, BoxScaleLoss=5.982, ClassLoss=25.264\n",
            "[Epoch 0][Batch 23], LR: 1.00E-03, Speed: 14.338 samples/sec, ObjLoss=1691.507, BoxCenterLoss=18.278, BoxScaleLoss=5.900, ClassLoss=24.981\n",
            "[Epoch 0][Batch 24], LR: 1.00E-03, Speed: 10.004 samples/sec, ObjLoss=1644.590, BoxCenterLoss=18.275, BoxScaleLoss=5.836, ClassLoss=24.858\n",
            "[Epoch 0][Batch 25], LR: 1.00E-03, Speed: 14.179 samples/sec, ObjLoss=1600.112, BoxCenterLoss=18.260, BoxScaleLoss=5.815, ClassLoss=24.728\n",
            "[Epoch 0][Batch 26], LR: 1.00E-03, Speed: 10.258 samples/sec, ObjLoss=1557.647, BoxCenterLoss=18.192, BoxScaleLoss=5.738, ClassLoss=24.514\n",
            "[Epoch 0][Batch 27], LR: 1.00E-03, Speed: 14.218 samples/sec, ObjLoss=1517.417, BoxCenterLoss=18.183, BoxScaleLoss=5.706, ClassLoss=24.394\n",
            "[Epoch 0][Batch 28], LR: 1.00E-03, Speed: 9.558 samples/sec, ObjLoss=1479.201, BoxCenterLoss=18.165, BoxScaleLoss=5.641, ClassLoss=24.255\n",
            "[Epoch 0][Batch 29], LR: 1.00E-03, Speed: 14.771 samples/sec, ObjLoss=1442.693, BoxCenterLoss=18.141, BoxScaleLoss=5.571, ClassLoss=24.120\n",
            "[Epoch 0][Batch 30], LR: 1.00E-03, Speed: 14.985 samples/sec, ObjLoss=1407.887, BoxCenterLoss=18.071, BoxScaleLoss=5.519, ClassLoss=23.959\n",
            "[Epoch 0] Training cost: 43.674, ObjLoss=1407.887, BoxCenterLoss=18.071, BoxScaleLoss=5.519, ClassLoss=23.959\n",
            "[Epoch 0] Validation: \n",
            "bar_clamp=0.14787579152348418\n",
            "gear_box=0.0029644268774703555\n",
            "vase=0.373366943134385\n",
            "part_1=0.0\n",
            "part_3=0.09844227261739952\n",
            "nozzle=0.0\n",
            "pawn=0.0\n",
            "turbine_housing=0.0\n",
            "mAP=0.07783117926909239\n",
            "drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints\n",
            "drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints/yolo/SAN-349/yolo3_darknet53_coco_BE_0000_map_0.0778.params\n",
            "Best map:  0.07783117926909239\n",
            "[Epoch 1][Batch 0], LR: 1.00E-03, Speed: 4.364 samples/sec, ObjLoss=1374.712, BoxCenterLoss=18.036, BoxScaleLoss=5.495, ClassLoss=23.833\n",
            "[Epoch 1][Batch 1], LR: 1.00E-03, Speed: 13.588 samples/sec, ObjLoss=1343.073, BoxCenterLoss=18.040, BoxScaleLoss=5.460, ClassLoss=23.770\n",
            "[Epoch 1][Batch 2], LR: 1.00E-03, Speed: 14.443 samples/sec, ObjLoss=1312.743, BoxCenterLoss=17.987, BoxScaleLoss=5.417, ClassLoss=23.630\n",
            "[Epoch 1][Batch 3], LR: 1.00E-03, Speed: 14.376 samples/sec, ObjLoss=1283.771, BoxCenterLoss=17.944, BoxScaleLoss=5.361, ClassLoss=23.541\n",
            "[Epoch 1][Batch 4], LR: 1.00E-03, Speed: 8.774 samples/sec, ObjLoss=1256.071, BoxCenterLoss=17.954, BoxScaleLoss=5.334, ClassLoss=23.488\n",
            "[Epoch 1][Batch 5], LR: 1.00E-03, Speed: 13.949 samples/sec, ObjLoss=1229.498, BoxCenterLoss=17.940, BoxScaleLoss=5.294, ClassLoss=23.423\n",
            "[Epoch 1][Batch 6], LR: 1.00E-03, Speed: 10.676 samples/sec, ObjLoss=1204.003, BoxCenterLoss=17.914, BoxScaleLoss=5.258, ClassLoss=23.354\n",
            "[Epoch 1][Batch 7], LR: 1.00E-03, Speed: 13.732 samples/sec, ObjLoss=1179.557, BoxCenterLoss=17.889, BoxScaleLoss=5.242, ClassLoss=23.277\n",
            "[Epoch 1][Batch 8], LR: 1.00E-03, Speed: 11.057 samples/sec, ObjLoss=1156.054, BoxCenterLoss=17.815, BoxScaleLoss=5.212, ClassLoss=23.159\n",
            "[Epoch 1][Batch 9], LR: 1.00E-03, Speed: 13.255 samples/sec, ObjLoss=1133.581, BoxCenterLoss=17.784, BoxScaleLoss=5.182, ClassLoss=23.072\n",
            "[Epoch 1][Batch 10], LR: 1.00E-03, Speed: 14.217 samples/sec, ObjLoss=1111.860, BoxCenterLoss=17.720, BoxScaleLoss=5.156, ClassLoss=22.963\n",
            "[Epoch 1][Batch 11], LR: 1.00E-03, Speed: 10.570 samples/sec, ObjLoss=1090.990, BoxCenterLoss=17.699, BoxScaleLoss=5.128, ClassLoss=22.940\n",
            "[Epoch 1][Batch 12], LR: 1.00E-03, Speed: 12.672 samples/sec, ObjLoss=1070.813, BoxCenterLoss=17.618, BoxScaleLoss=5.082, ClassLoss=22.815\n",
            "[Epoch 1][Batch 13], LR: 1.00E-03, Speed: 10.807 samples/sec, ObjLoss=1051.468, BoxCenterLoss=17.628, BoxScaleLoss=5.067, ClassLoss=22.823\n",
            "[Epoch 1][Batch 14], LR: 1.00E-03, Speed: 12.927 samples/sec, ObjLoss=1032.806, BoxCenterLoss=17.636, BoxScaleLoss=5.045, ClassLoss=22.783\n",
            "[Epoch 1][Batch 15], LR: 1.00E-03, Speed: 12.430 samples/sec, ObjLoss=1014.770, BoxCenterLoss=17.578, BoxScaleLoss=5.023, ClassLoss=22.683\n",
            "[Epoch 1][Batch 16], LR: 1.00E-03, Speed: 12.754 samples/sec, ObjLoss=997.353, BoxCenterLoss=17.492, BoxScaleLoss=4.997, ClassLoss=22.552\n",
            "[Epoch 1][Batch 17], LR: 1.00E-03, Speed: 11.287 samples/sec, ObjLoss=980.572, BoxCenterLoss=17.480, BoxScaleLoss=4.976, ClassLoss=22.511\n",
            "[Epoch 1][Batch 18], LR: 1.00E-03, Speed: 14.031 samples/sec, ObjLoss=964.388, BoxCenterLoss=17.473, BoxScaleLoss=4.964, ClassLoss=22.479\n",
            "[Epoch 1][Batch 19], LR: 1.00E-03, Speed: 11.162 samples/sec, ObjLoss=948.686, BoxCenterLoss=17.388, BoxScaleLoss=4.942, ClassLoss=22.364\n",
            "[Epoch 1][Batch 20], LR: 1.00E-03, Speed: 11.299 samples/sec, ObjLoss=933.500, BoxCenterLoss=17.348, BoxScaleLoss=4.941, ClassLoss=22.292\n",
            "[Epoch 1][Batch 21], LR: 1.00E-03, Speed: 12.956 samples/sec, ObjLoss=918.802, BoxCenterLoss=17.301, BoxScaleLoss=4.927, ClassLoss=22.226\n",
            "[Epoch 1][Batch 22], LR: 1.00E-03, Speed: 11.306 samples/sec, ObjLoss=904.581, BoxCenterLoss=17.286, BoxScaleLoss=4.922, ClassLoss=22.179\n",
            "[Epoch 1][Batch 23], LR: 1.00E-03, Speed: 13.331 samples/sec, ObjLoss=890.847, BoxCenterLoss=17.264, BoxScaleLoss=4.908, ClassLoss=22.152\n",
            "[Epoch 1][Batch 24], LR: 1.00E-03, Speed: 10.526 samples/sec, ObjLoss=877.501, BoxCenterLoss=17.267, BoxScaleLoss=4.891, ClassLoss=22.129\n",
            "[Epoch 1][Batch 25], LR: 1.00E-03, Speed: 13.737 samples/sec, ObjLoss=864.558, BoxCenterLoss=17.233, BoxScaleLoss=4.875, ClassLoss=22.072\n",
            "[Epoch 1][Batch 26], LR: 1.00E-03, Speed: 11.538 samples/sec, ObjLoss=852.007, BoxCenterLoss=17.235, BoxScaleLoss=4.854, ClassLoss=22.068\n",
            "[Epoch 1][Batch 27], LR: 1.00E-03, Speed: 11.835 samples/sec, ObjLoss=839.829, BoxCenterLoss=17.238, BoxScaleLoss=4.844, ClassLoss=22.057\n",
            "[Epoch 1][Batch 28], LR: 1.00E-03, Speed: 9.604 samples/sec, ObjLoss=827.976, BoxCenterLoss=17.231, BoxScaleLoss=4.842, ClassLoss=22.033\n",
            "[Epoch 1][Batch 29], LR: 1.00E-03, Speed: 14.147 samples/sec, ObjLoss=816.496, BoxCenterLoss=17.243, BoxScaleLoss=4.836, ClassLoss=22.030\n",
            "[Epoch 1][Batch 30], LR: 1.00E-03, Speed: 14.941 samples/sec, ObjLoss=805.318, BoxCenterLoss=17.193, BoxScaleLoss=4.821, ClassLoss=21.958\n",
            "[Epoch 1] Training cost: 43.076, ObjLoss=805.318, BoxCenterLoss=17.193, BoxScaleLoss=4.821, ClassLoss=21.958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Invalid metric value: nan for channel prec_by_class_val_part_1. Metrics with nan or +/-inf values will not be sent to server\n",
            "Invalid metric value: nan for channel prec_by_class_val_nozzle. Metrics with nan or +/-inf values will not be sent to server\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1] Validation: \n",
            "bar_clamp=0.42360577929338533\n",
            "gear_box=0.021711012564671103\n",
            "vase=0.18966912326067256\n",
            "part_1=0.0\n",
            "part_3=0.12816744533000607\n",
            "nozzle=0.0\n",
            "pawn=0.045454545454545456\n",
            "turbine_housing=0.09090909090909091\n",
            "mAP=0.11243962460154644\n",
            "drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints\n",
            "drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints/yolo/SAN-349/yolo3_darknet53_coco_BE_0001_map_0.1124.params\n",
            "Best map:  0.11243962460154644\n",
            "[Epoch 2][Batch 0], LR: 1.00E-03, Speed: 4.339 samples/sec, ObjLoss=794.444, BoxCenterLoss=17.161, BoxScaleLoss=4.799, ClassLoss=21.903\n",
            "[Epoch 2][Batch 1], LR: 1.00E-03, Speed: 14.197 samples/sec, ObjLoss=783.878, BoxCenterLoss=17.142, BoxScaleLoss=4.784, ClassLoss=21.878\n",
            "[Epoch 2][Batch 2], LR: 1.00E-03, Speed: 14.058 samples/sec, ObjLoss=773.608, BoxCenterLoss=17.160, BoxScaleLoss=4.777, ClassLoss=21.900\n",
            "[Epoch 2][Batch 3], LR: 1.00E-03, Speed: 14.168 samples/sec, ObjLoss=763.620, BoxCenterLoss=17.140, BoxScaleLoss=4.762, ClassLoss=21.863\n",
            "[Epoch 2][Batch 4], LR: 1.00E-03, Speed: 9.999 samples/sec, ObjLoss=753.890, BoxCenterLoss=17.128, BoxScaleLoss=4.746, ClassLoss=21.829\n",
            "[Epoch 2][Batch 5], LR: 1.00E-03, Speed: 13.755 samples/sec, ObjLoss=744.416, BoxCenterLoss=17.136, BoxScaleLoss=4.738, ClassLoss=21.830\n",
            "[Epoch 2][Batch 6], LR: 1.00E-03, Speed: 9.264 samples/sec, ObjLoss=735.185, BoxCenterLoss=17.127, BoxScaleLoss=4.718, ClassLoss=21.809\n",
            "[Epoch 2][Batch 7], LR: 1.00E-03, Speed: 13.593 samples/sec, ObjLoss=726.201, BoxCenterLoss=17.134, BoxScaleLoss=4.700, ClassLoss=21.805\n",
            "[Epoch 2][Batch 8], LR: 1.00E-03, Speed: 10.749 samples/sec, ObjLoss=717.430, BoxCenterLoss=17.152, BoxScaleLoss=4.695, ClassLoss=21.816\n",
            "[Epoch 2][Batch 9], LR: 1.00E-03, Speed: 13.644 samples/sec, ObjLoss=708.859, BoxCenterLoss=17.125, BoxScaleLoss=4.676, ClassLoss=21.786\n",
            "[Epoch 2][Batch 10], LR: 1.00E-03, Speed: 10.364 samples/sec, ObjLoss=700.521, BoxCenterLoss=17.144, BoxScaleLoss=4.664, ClassLoss=21.807\n",
            "[Epoch 2][Batch 11], LR: 1.00E-03, Speed: 13.626 samples/sec, ObjLoss=692.367, BoxCenterLoss=17.139, BoxScaleLoss=4.654, ClassLoss=21.799\n",
            "[Epoch 2][Batch 12], LR: 1.00E-03, Speed: 10.467 samples/sec, ObjLoss=684.401, BoxCenterLoss=17.126, BoxScaleLoss=4.639, ClassLoss=21.778\n",
            "[Epoch 2][Batch 13], LR: 1.00E-03, Speed: 12.460 samples/sec, ObjLoss=676.650, BoxCenterLoss=17.138, BoxScaleLoss=4.630, ClassLoss=21.786\n",
            "[Epoch 2][Batch 14], LR: 1.00E-03, Speed: 12.373 samples/sec, ObjLoss=669.027, BoxCenterLoss=17.134, BoxScaleLoss=4.607, ClassLoss=21.777\n",
            "[Epoch 2][Batch 15], LR: 1.00E-03, Speed: 12.710 samples/sec, ObjLoss=661.599, BoxCenterLoss=17.126, BoxScaleLoss=4.587, ClassLoss=21.767\n",
            "[Epoch 2][Batch 16], LR: 1.00E-03, Speed: 10.271 samples/sec, ObjLoss=654.364, BoxCenterLoss=17.116, BoxScaleLoss=4.574, ClassLoss=21.749\n",
            "[Epoch 2][Batch 17], LR: 1.00E-03, Speed: 13.834 samples/sec, ObjLoss=647.264, BoxCenterLoss=17.115, BoxScaleLoss=4.564, ClassLoss=21.749\n",
            "[Epoch 2][Batch 18], LR: 1.00E-03, Speed: 11.437 samples/sec, ObjLoss=640.323, BoxCenterLoss=17.118, BoxScaleLoss=4.545, ClassLoss=21.749\n",
            "[Epoch 2][Batch 19], LR: 1.00E-03, Speed: 11.491 samples/sec, ObjLoss=633.530, BoxCenterLoss=17.097, BoxScaleLoss=4.529, ClassLoss=21.732\n",
            "[Epoch 2][Batch 20], LR: 1.00E-03, Speed: 11.768 samples/sec, ObjLoss=626.923, BoxCenterLoss=17.089, BoxScaleLoss=4.522, ClassLoss=21.727\n",
            "[Epoch 2][Batch 21], LR: 1.00E-03, Speed: 11.474 samples/sec, ObjLoss=620.455, BoxCenterLoss=17.086, BoxScaleLoss=4.517, ClassLoss=21.723\n",
            "[Epoch 2][Batch 22], LR: 1.00E-03, Speed: 11.665 samples/sec, ObjLoss=614.105, BoxCenterLoss=17.093, BoxScaleLoss=4.511, ClassLoss=21.729\n",
            "[Epoch 2][Batch 23], LR: 1.00E-03, Speed: 10.934 samples/sec, ObjLoss=607.897, BoxCenterLoss=17.073, BoxScaleLoss=4.503, ClassLoss=21.710\n",
            "[Epoch 2][Batch 24], LR: 1.00E-03, Speed: 13.757 samples/sec, ObjLoss=601.794, BoxCenterLoss=17.059, BoxScaleLoss=4.493, ClassLoss=21.694\n",
            "[Epoch 2][Batch 25], LR: 1.00E-03, Speed: 10.384 samples/sec, ObjLoss=595.822, BoxCenterLoss=17.020, BoxScaleLoss=4.479, ClassLoss=21.658\n",
            "[Epoch 2][Batch 26], LR: 1.00E-03, Speed: 13.888 samples/sec, ObjLoss=589.980, BoxCenterLoss=16.988, BoxScaleLoss=4.471, ClassLoss=21.629\n",
            "[Epoch 2][Batch 27], LR: 1.00E-03, Speed: 12.369 samples/sec, ObjLoss=584.271, BoxCenterLoss=16.968, BoxScaleLoss=4.458, ClassLoss=21.623\n",
            "[Epoch 2][Batch 28], LR: 1.00E-03, Speed: 12.145 samples/sec, ObjLoss=578.657, BoxCenterLoss=16.958, BoxScaleLoss=4.449, ClassLoss=21.616\n",
            "[Epoch 2][Batch 29], LR: 1.00E-03, Speed: 10.361 samples/sec, ObjLoss=573.185, BoxCenterLoss=16.934, BoxScaleLoss=4.439, ClassLoss=21.594\n",
            "[Epoch 2][Batch 30], LR: 1.00E-03, Speed: 14.895 samples/sec, ObjLoss=567.813, BoxCenterLoss=16.923, BoxScaleLoss=4.438, ClassLoss=21.593\n",
            "[Epoch 2] Training cost: 43.682, ObjLoss=567.813, BoxCenterLoss=16.923, BoxScaleLoss=4.438, ClassLoss=21.593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Invalid metric value: nan for channel prec_by_class_val_part_1. Metrics with nan or +/-inf values will not be sent to server\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 2] Validation: \n",
            "bar_clamp=0.4181970044174167\n",
            "gear_box=0.012006861063464836\n",
            "vase=0.2724747630647913\n",
            "part_1=0.0\n",
            "part_3=0.11119128256532836\n",
            "nozzle=0.045454545454545456\n",
            "pawn=0.03208556149732621\n",
            "turbine_housing=0.045454545454545456\n",
            "mAP=0.11710807043967728\n",
            "drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints\n",
            "drive/My Drive/UFBA/Doutorado/Ssd_test/checkpoints/yolo/SAN-349/yolo3_darknet53_coco_BE_0002_map_0.1171.params\n",
            "Best map:  0.11710807043967728\n",
            "[Epoch 3][Batch 0], LR: 1.00E-03, Speed: 4.121 samples/sec, ObjLoss=562.533, BoxCenterLoss=16.921, BoxScaleLoss=4.437, ClassLoss=21.596\n",
            "[Epoch 3][Batch 1], LR: 1.00E-03, Speed: 13.895 samples/sec, ObjLoss=557.349, BoxCenterLoss=16.917, BoxScaleLoss=4.425, ClassLoss=21.609\n",
            "[Epoch 3][Batch 2], LR: 1.00E-03, Speed: 14.305 samples/sec, ObjLoss=552.239, BoxCenterLoss=16.879, BoxScaleLoss=4.411, ClassLoss=21.573\n",
            "[Epoch 3][Batch 3], LR: 1.00E-03, Speed: 13.491 samples/sec, ObjLoss=547.249, BoxCenterLoss=16.868, BoxScaleLoss=4.401, ClassLoss=21.564\n",
            "[Epoch 3][Batch 4], LR: 1.00E-03, Speed: 12.924 samples/sec, ObjLoss=542.342, BoxCenterLoss=16.856, BoxScaleLoss=4.388, ClassLoss=21.557\n",
            "[Epoch 3][Batch 5], LR: 1.00E-03, Speed: 14.319 samples/sec, ObjLoss=537.530, BoxCenterLoss=16.833, BoxScaleLoss=4.376, ClassLoss=21.530\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7585b4d457c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mtrain_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0mtrain_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-28575cc7caf4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m           \u001b[0;31m# First create the trainer. Obs: you should reset_ctx before creating the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_type\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'faster'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaster_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-28575cc7caf4>\u001b[0m in \u001b[0;36myolo_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                     \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 \u001b[0mobj_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mcenter_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0mscale_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/metric.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, _, preds)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_metric\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_sum_metric\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}